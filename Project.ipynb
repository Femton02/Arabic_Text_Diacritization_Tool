{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Seif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\Seif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Seif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, FreqDist\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "ARABIC_LETTERS_LIST = [\n",
    "    'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط',\n",
    "    'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي'\n",
    "]\n",
    "\n",
    "DIACRITICS_LIST = [\n",
    "    'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ'\n",
    "]\n",
    "\n",
    "ALL_CHARACTERS = ARABIC_LETTERS_LIST + DIACRITICS_LIST\n",
    "\n",
    "\n",
    "CHAR_TO_INDEX = {char: index for index, char in enumerate(ALL_CHARACTERS)}\n",
    "\n",
    "INDEX_TO_CHAR = {index: char for index, char in enumerate(ALL_CHARACTERS)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, CuDNNLSTM, Bidirectional, TimeDistributed\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Activation\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaningEnglishAndNumbers(text): \n",
    "    text = re.sub(r\"[a-zA-Z0-9٠-٩]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleaningSpaces(text):\n",
    "    text = re.sub(r\"[\\s]+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def Remove(text):\n",
    "    text = re.sub(r\"[[]]\", \"\", text)\n",
    "    text = re.sub(r\"[]]\", \"\", text)\n",
    "    text = re.sub(r\"[[]\", \"\", text)\n",
    "    text = re.sub(r\"-\", \"\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"[،؛؟():.]\", \"\", text)\n",
    "    text = re.sub(r\"[{}}!*;»«]\", \"\", text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def RemoveSingleLetters(text):\n",
    "    #remove single letters [أ-ي] boundered with spaces\n",
    "    text = re.sub(r\"\\s[أ-ي]\\s\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train raws: 910274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seif\\AppData\\Local\\Temp\\ipykernel_11868\\1934813480.py:12: FutureWarning: Possible nested set at position 1\n",
      "  text = re.sub(r\"[[]]\", \"\", text)\n",
      "C:\\Users\\Seif\\AppData\\Local\\Temp\\ipykernel_11868\\1934813480.py:14: FutureWarning: Possible nested set at position 1\n",
      "  text = re.sub(r\"[[]\", \"\", text)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'dataset/val.txt'\n",
    "\n",
    "def read_file_content(file_path, encoding='utf-8'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        return file.read()\n",
    "\n",
    "file_content = read_file_content(file_path)\n",
    "\n",
    "#cleaning text\n",
    "\n",
    "file_content = cleaningEnglishAndNumbers(file_content)\n",
    "file_content = cleaningSpaces(file_content)\n",
    "file_content = Remove(file_content)\n",
    "file_content = RemoveSingleLetters(file_content)\n",
    "\n",
    "\n",
    "# add it in txt file\n",
    "with open('dataset/Updatedtrain.txt', 'w', encoding='utf-8') as file: \n",
    "    file.write(file_content)\n",
    "\n",
    "print('Train raws:', len(file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train raws without diactrics: 542752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_diactrics(text):\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text) \n",
    "    return text\n",
    "\n",
    "file_content_without_diactrics = remove_diactrics(file_content)\n",
    "\n",
    "with open('dataset/UpdatedtrainWithoutDiactrics.txt', 'w', encoding='utf-8') as file: \n",
    "    file.write(file_content_without_diactrics)\n",
    "\n",
    "\n",
    "print('Train raws without diactrics:', len(file_content_without_diactrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    return [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "\n",
    "file_content_tokenized = tokenize(file_content_without_diactrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 910274\n",
      "Train rows without diacritics: 542752\n"
     ]
    }
   ],
   "source": [
    "#using bag of words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(file_content_tokenized)\n",
    "\n",
    "with open('dataset/BoW_representation.txt', 'w', encoding='utf-8') as file:\n",
    "    for row in X.toarray():\n",
    "        file.write(\" \".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print('Train rows:', len(file_content))\n",
    "print('Train rows without diacritics:', len(file_content_without_diactrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 910274\n"
     ]
    }
   ],
   "source": [
    "#using BERT embeddings\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "encoded_texts = [tokenizer.encode(sentence, add_special_tokens=True) for sentence in file_content_tokenized]\n",
    "\n",
    "with open('dataset/BERT_embeddings.txt', 'w', encoding='utf-8') as file:\n",
    "    for embeddings in encoded_texts:\n",
    "        file.write(\" \".join(map(str, embeddings)) + \"\\n\")\n",
    "\n",
    "print('Train rows:', len(file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
